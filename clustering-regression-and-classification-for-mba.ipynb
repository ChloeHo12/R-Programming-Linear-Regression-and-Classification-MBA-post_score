{"metadata":{"kernelspec":{"name":"ir","display_name":"R","language":"R"},"language_info":{"name":"R","codemirror_mode":"r","pygments_lexer":"r","mimetype":"text/x-r-source","file_extension":".r","version":"4.0.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Introduction\n\nMBA is the common abbreviation for a Master of Business Administration degree. An MBA is a common stepping stone to C-suite positions in large businesses, as well as provides aspiring entrepreneurs a springboard to success. This project analyzes how a person’s MBA post-score is influenced by lower-level performances and demographics. ","metadata":{"_uuid":"051d70d956493feee0c6d64651c6a088724dca2a","_execution_state":"idle"}},{"cell_type":"code","source":"# Load the dplyr library\nlibrary(dplyr)\nlibrary(tibble)\n\n# dplyr is a part of the R \"tidyverse\"\nlibrary(tidyverse)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:16.315144Z","iopub.execute_input":"2022-06-10T06:25:16.318969Z","iopub.status.idle":"2022-06-10T06:25:17.267607Z"},"_kg_hide-input":true,"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# read csv file\nadmissions <- read.csv(\"../input/others/MBA_ADMISSIONS.csv\")\nhead(admissions)\n","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:20.134251Z","iopub.execute_input":"2022-06-10T06:25:20.164980Z","iopub.status.idle":"2022-06-10T06:25:20.219455Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":" ***1.1 Basic Visualization***","metadata":{}},{"cell_type":"code","source":"# Bar charts showing the distribution of Age, Gender&Marital Status\nbarplot(table(admissions$Age), ylim = c(0,170), main = 'Distribution of Age', ylab = \"Percentage\", xlab = \"Age in years\", cex.lab = 1.2, cex.name = 1,xlim = c(0,10), cex.axis = 1.2, beside = TRUE, col = c(10,11))\nbarplot(table(admissions$Gender, admissions$Marital_status), ylim = c(0,300), main = 'Distribution of Marital Status and Gender', ylab = \"Percentage\", xlab = \"Count\", cex.lab = 1.2, cex.name = 1, xlim = c(0,10), cex.axis = 1.2, legend = TRUE, beside = TRUE, col = c(10,11))","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:30.452434Z","iopub.execute_input":"2022-06-10T06:25:30.453976Z","iopub.status.idle":"2022-06-10T06:25:30.617609Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"> Figure 1: Upper: Distribution of Age, Lower: Distribution of gender and Marital Status\n\nFigure 1 shows that most MBA learners are around 22 years old and single. Interestingly, while the number of single Male MBAs outweigh that of female MBAs, there are more married Female MBA Leaners","metadata":{}},{"cell_type":"code","source":"# Pie charts showing the distribution of Specilization and Previous Degrees\npie(table(admissions$Specialization), main = \"Distribution of Specialization\", col = c(3,4), cex = 1.3, radius = 1.2)\npie(table(admissions$Previous_Degree), main = \"Distribution of Previous Degrees\", col = c(3,4), cex = 1.3, radius = 1.2)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:35.494018Z","iopub.execute_input":"2022-06-10T06:25:35.496661Z","iopub.status.idle":"2022-06-10T06:25:35.660246Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"> Figure 2: Upper: Distribution of Previous Degree; Lower: Distribution of Specialization\n\nFigure 2 shows that roughly a half of MBA Leaners pursued Engineering Degree in Undergraduate, followed by Commerce Major which makes up around 25%. Pursing MBA, most learners choose to specialize in Marketing, followed by Finance, LOS, and HR respectively.","metadata":{}},{"cell_type":"code","source":"# Box plots showing the distribution of MBA pre-scores\nboxplot(admissions$pre_score,\n        main = \"Pre-scores of MBA Applicants\",\n        xlab = \"MBA Applicants\",\n        ylab = \"Post-score\",\n        col = \"pink\",\n        border = \"blue\",\n        notch = TRUE)\n# Box plots showing the distribution of MBA post-scores\nboxplot(admissions$post_score,\n        main = \"Post-scores of MBA Applicants\",\n        xlab = \"MBA Applicants\",\n        ylab = \"Post-score\",\n        col = \"pink\",\n        border = \"blue\",\n        notch = TRUE)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:42.017271Z","iopub.execute_input":"2022-06-10T06:25:42.022575Z","iopub.status.idle":"2022-06-10T06:25:42.265713Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"> Figure 3: Upper: Distribution of Pre-score; Lower: Distribution of Post-score\n\nFigure 3 shows the distribution of MBAs’ pre-scores and post-scores. Regarding pre-score, the median pre-score is 68 over the scale of 100 (68/100), and around 75% of the MBAs get the pre-score higher than 60. Regarding post-score, the median post-score is around 78/100, and around 75% of the MBAs score higher than 70.","metadata":{}},{"cell_type":"markdown","source":"***1.2 Summary of methods***","metadata":{}},{"cell_type":"markdown","source":"In this report, I am going to use Clustering, Regression, and Classification to analyze the MBA.csv data set.\n\n> Clustering: K-means Clustering Method\n\n\nPerforming K-means Clustering Method, I define a target number k, which refers to the number of centroids that I need in the dataset. A centroid is the imaginary or real location representing the center of the cluster. Then, I will allocate every data point to the nearest cluster, while keeping the centroids as small as possible.\n-\tK = 7\n-\tIncluded variables: pre_score, Percentage_in_10_Class, Percentage_in_12_Class, Percentage_in_Under_Graduate, percentage_MBA\n\n> Linear Regression: \n\nI will take the admission data and calculate the effect that the independent variables (potential predictors listed below) have on the response variable post_score using the equation for the linear model: lm().\n-\tResponse variable: post_score\n-\tPredictors selection: using forward selection, which begins with an empty model and adds in variables one by one. In each forward step, I will add the one variable that gives the single best improvement to my model\n\n> Classification: Logistic Regression\n\nI will split the admission dataset into a training set to train the model on and a testing set to test the model on. Then, I will predict test data based on model and evaluate model accuracy.\n\n-\tBinary response variable: Performance (converted by post_score) \n\n  * Performance = 0 means this is a high MBA percentage\n  * Performance = 1 means this is a low MBA percentage","metadata":{}},{"cell_type":"markdown","source":"# 2. Models","metadata":{}},{"cell_type":"markdown","source":"***2.1 K-means clustering***\n\nSince Admissions.csv is a large data set with many variables, it is not ideal use Hierarchical clustering which is generally applicable to a small set of data. Therefore, I choose K-means clustering for the data set – which is more efficient.\n\nTo use K-Means, we need to specify the value of K that is the number of clusters we want to group our data into. To determine K, I am going to use Elbow Curve Method.","metadata":{}},{"cell_type":"code","source":"admission_num <- (admissions[, -c(8:14)])\nK_list <- 2:30\ntotal_dist <- 0\nfor (k in K_list){\n  kmeans_num_choose <- kmeans(admission_num, k)\n  total_dist <- c(total_dist, kmeans_num_choose$tot.withinss)\n}\nplot(total_dist[-1])","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:25:56.282792Z","iopub.execute_input":"2022-06-10T06:25:56.284376Z","iopub.status.idle":"2022-06-10T06:25:56.434258Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"> Figure 4: Elbow Curve for Kmeans\n\nFigure 4 shows that at K = 7, the sum of squared distance begins to flatten out and we can see an inflection point. Therefore, K = 7 is a good choice for the number of clusters.","metadata":{}},{"cell_type":"markdown","source":"For K-means clustering model, I eliminated all non-numerical variables and chose Manhattan Distance for measurement. Specifically, Figure 5 shows the result from K-means clustering, which is a scatter plot of different clusters in accordance to MBA post-score (denoted: post_score) and Undergraduate Performance (denoted: Percentage_in_Under_Graduate).","metadata":{}},{"cell_type":"code","source":"admissions_kmeans <- kmeans(admission_num, 7)\nfor (i in 1:7){\n  if (i == 1){\n    plot(admissions[which(admissions_kmeans$cluster == i), c('Percentage_in_Under_Graduate', 'post_score')],\n         xlim = c(min(admissions$Percentage_in_Under_Graduate), max(admissions$Percentage_in_Under_Graduate)),\n         ylim = c(min(admissions$post_score), max(admissions$post_score)))\n  }else{\n    points(admissions[which(admissions_kmeans$cluster == i), c('Percentage_in_Under_Graduate', 'post_score')], col = i)\n  }\n}","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:26:02.733602Z","iopub.execute_input":"2022-06-10T06:26:02.735204Z","iopub.status.idle":"2022-06-10T06:26:02.856656Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"> Figure 5: Clusters in terms of MBA Post-score and Undergraduate Performance","metadata":{}},{"cell_type":"markdown","source":"***2.2 Linear Regression***\n\n* Variable selection\n\nI have used a linear regression model to predict MBA admissions based on the most relevant predictors by utilizing forward selection. Based on forward selection results with the response variable post_score, I picked 9 potential predictors that improve the model as I continued the process by adding one more variable at a time. ","metadata":{}},{"cell_type":"code","source":"library(MASS)\ndata_null <- lm(post_score~1., admissions)\ndata_full <- lm(post_score~., admissions)\n\nforward_lm <- stepAIC(data_null, direction = \"forward\",\n                      scope = list(upper = data_full, lower = data_null))\nsummary(forward_lm)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:26:07.800139Z","iopub.execute_input":"2022-06-10T06:26:07.801673Z","iopub.status.idle":"2022-06-10T06:26:08.099733Z"},"_kg_hide-input":true,"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Potential predictors are: *perceived.Job.Skill, Percentage_in_Under_Graduate, Percentage_in_10_Class, STATE, Gender*","metadata":{}},{"cell_type":"code","source":"pairs(admission_num)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:26:21.814651Z","iopub.execute_input":"2022-06-10T06:26:21.816192Z","iopub.status.idle":"2022-06-10T06:26:22.678954Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"> Figure 6: Variables of pairs \n\nFigure 6 shows a pairs plot which explains the pairwise relationship between different variables in a dataset. ","metadata":{}},{"cell_type":"code","source":"# resuts from linear regression\nadmissions_lm <- lm(post_score ~ perceived.Job.Skill + Percentage_in_Under_Graduate + Percentage_in_10_Class + STATE + Gender, data = admissions) \nsummary(admissions_lm)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:26:26.588381Z","iopub.execute_input":"2022-06-10T06:26:26.589936Z","iopub.status.idle":"2022-06-10T06:26:26.614940Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"* Full model based off response variable and 5 potential variables\n\n**post_score = 71.72 - 5.84 * perceived.Job.Skillprefered skills - 12.69 * perceived.Job.Skillrequired skills + 0.4 * Percentage_in_Under_Graduate - 0.26 * Percentage_in_10_Class - 3.48 * STATEEast Zone + 18.52 * STATENorth East -  1.35 * STATENorth Zone - 1.11 * STATESouth Zone - 9.11 * STATEWest Zone + 3.53 * GenderMale**","metadata":{}},{"cell_type":"markdown","source":"* Interaction between predictors","metadata":{}},{"cell_type":"code","source":"lm_int <- lm(formula = post_score ~ Percentage_in_Under_Graduate*Gender, data = admissions)\nsummary(lm_int)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:26:34.264142Z","iopub.execute_input":"2022-06-10T06:26:34.265640Z","iopub.status.idle":"2022-06-10T06:26:34.285828Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"I suspect there would be interaction between 2 predictors Percentage_in_Under_Graduate and Gender since these two variables are statistically significant, and therefore have large effects on the outcome (post_score). Moreover, I suspect that gender may influence their undergraduate perfomance.","metadata":{}},{"cell_type":"markdown","source":"**post_score = 64.83 + 0.13 * Percentage_in_Under_Graduate - 25.9 * GenderMale + 0.4 * (Percentage_in_Under_Graduate * GenderMale)**\n\nBased on the model, it can be interpreted that Under-graduate performance has a positive impact on post-score, however this impact is worse if the person is a Male MBA. Being a Male MBA has a negative impact on post-score comparing to Female MBA; yet the post-score would be higher if the Male MBA performed well in their Under-graduate studies. ","metadata":{}},{"cell_type":"markdown","source":"***2.3 Logistic Regression***\n\nI chose Logistic Regression for Classification because this provides p-values, standard errors, etc, which gives me insight into what features are important and what they do. Using Logistic classification, I created Performance as the binary response variable, in which Performance is determined by:\n* If post_score < 70: Performance = 0, which is low MBA performance  \n* If post_score > 70: Performance = 1, which is high MBA performance","metadata":{}},{"cell_type":"code","source":"# Split the data set into training set and validation set\nadmissions <- read.csv(\"../input/others/MBA_ADMISSIONS.csv\")\noptions(scipen=200)\nadmissions <- na.omit(admissions)\nadmissions <- mutate(admissions, Performance = ifelse(admissions$post_score > 70, 1, 0))\nadmissions <- (admissions[, -c(8:14)])\nadmissions <- subset(admissions, select = -post_score)\n\ntraining <- sample(1:nrow(admissions), 0.8*nrow(admissions))\ntrainingset <- admissions[training,]\nvalidation <- setdiff(1:nrow(admissions), training)\nvalidationset <- admissions[validation,]\n\n# Fit the model using generalized linear model\nadmission_log <- glm(Performance~., trainingset, family = \"binomial\")\nsummary(admission_log)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:28:25.151106Z","iopub.execute_input":"2022-06-10T06:28:25.152678Z","iopub.status.idle":"2022-06-10T06:28:25.205492Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"**Intepretation**\n\nIt can be seen that only 4 out of 6 predictors are significantly associated to the outcome. These include: pre_score, Percentage_in_10_Class, Percentage_in_Under_Graduate, percentage_MBA.\n\nThe coefficient estimate of the variable pre_score is b = 0.050989, which is positive. This means that a higher MBA pre_score is associated with an increase in the probability of getting High MBA performance. However the coefficient for the variable Percentage_in_10_Class is b =  -0.066698, which is negative. This means that an increase in Grade 10 score will be associated with a decreased probability of getting a high MBA performance. From the logistic regression results, it can be noticed that some variables (Age_in_years, Percentage_in_12_Class) are not statistically significant. Keeping them in the model may contribute to overfitting. Therefore, they should be eliminated. This can be done automatically using statistical techniques, selecting an optimal model with a reduced set of variables, without compromising the model accuracy.","metadata":{}},{"cell_type":"code","source":"admission_log <- glm(Performance~pre_score + Percentage_in_10_Class + Percentage_in_Under_Graduate + percentage_MBA, trainingset, family = \"binomial\")\nsummary(admission_log)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:29:29.449192Z","iopub.execute_input":"2022-06-10T06:29:29.450883Z","iopub.status.idle":"2022-06-10T06:29:29.482791Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"**Making predictions**\n\nI will make predictions using the validation data set in order to evaluate the performance of my logistic regression model. \nTo do this, I will follow the procedure:\n*   Predict the probabilities of getting high MBA performance\n*   Predict the class of individuals","metadata":{}},{"cell_type":"markdown","source":"> Predict the probabilities of getting high MBA performance\n","metadata":{}},{"cell_type":"code","source":"# Predict the probabilities of getting high MBA performance\nadmission_log_pred <- predict(admission_log, validationset, type = \"response\")\nhead(admission_log_pred)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:29:35.857875Z","iopub.execute_input":"2022-06-10T06:29:35.859470Z","iopub.status.idle":"2022-06-10T06:29:35.881362Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"The output is the probability that the MBA performance will be good. We know that these values correspond to the probability of the validation data set to be high, rather than low, because R indicates 1 for \"high\" and 0 for \"low\". ","metadata":{}},{"cell_type":"markdown","source":"> Predict the class of individuals:\n\nTo do this, first, I have to categorizes individuals into two groups based on their predicted probabilities (p) of getting good performance. To find p, I need to execute a thredshold model.\n\nWith the optimal p, I will then predict the class of individuals.","metadata":{}},{"cell_type":"code","source":"# Thredshold list\nthreshold_list <- seq(0.05, 0.95, 0.1)\nAcc <- 0\nfor (p in threshold_list){\n  admission_log_performance <- ifelse(admission_log_pred >= p, 1, 0)\n  conf_mat <- table(validationset$Performance, admission_log_performance)\n  Acc <- c(Acc, sum(diag(conf_mat))/sum(conf_mat))\n}\nplot(threshold_list, Acc[-1],\n     ylab = \"Accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:29:42.919054Z","iopub.execute_input":"2022-06-10T06:29:42.920707Z","iopub.status.idle":"2022-06-10T06:29:43.006768Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"> Figure 8: Threshold list\nFigure 8 shows the accuracy on the validation set. At threshold 0.7, the model gives the optimal accuracy. Therefore, p = 0.7","metadata":{}},{"cell_type":"code","source":"# Predict the class of individuals\npredicted.admissions <- ifelse(admission_log_pred > 0.7, \"high\", \"low\")\nhead(predicted.admissions)","metadata":{"execution":{"iopub.status.busy":"2022-06-10T06:31:10.095797Z","iopub.execute_input":"2022-06-10T06:31:10.097489Z","iopub.status.idle":"2022-06-10T06:31:10.117307Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"# **3. Conclusion**","metadata":{}},{"cell_type":"markdown","source":"\nPerforming K-means clustering, linear regression, and logistic regression, I am able to analyze how a an MBA's post-score is influenced by lower-level performances and demographics. The analysis reveals these key findings:\n- There are 5 predictors that mostly influence the MBA post-score: perceived.Job.Skill, Percentage_in_Under_Graduate, Percentage_in_10_Class, STATE, and Gender.\n- Under-graduate performance has a positive impact on post-score, however this impact is worse if the person is a Male MBA. Being a Male MBA has a negative impact on post-score comparing to Female MBA; yet the post-score would be higher if the Male MBA performed well in their Under-graduate studies.\n- High MBA pre-score, high grades in Under-graduate and MBA studies are associated with an increase in the probability of getting a good MBA performance (high MBA post-score).\n\n","metadata":{}}]}